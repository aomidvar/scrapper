{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn4wfERV+1sShMc1cdrho3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aomidvar/scrapper/blob/main/Cert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zm2LJUz9Ki2H",
        "outputId": "b2554230-f8b9-43a9-d125-e9ba9e490bbf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-015bd31d00f4>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlist_of_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import bs4 as bs\n",
        "import json\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "list_of_att = []\n",
        "\n",
        "\n",
        "def link_extractor(page):\n",
        "    try:\n",
        "        r = requests.get('https://www.kb.cert.org/vuls/bypublished/desc/?page='+str(page))\n",
        "    except requests.exceptions.ProxyError:\n",
        "        r = requests.get('https://www.kb.cert.org/vuls/bypublished/desc/?page='+str(page),proxies={'http':f'localhost:{random.randint(5002,5016)}','https':f'localhost:{random.randint(5002,5016)}'})\n",
        "    soup = BeautifulSoup(r.text, 'lxml')\n",
        "    table = soup.find(\"table\", attrs={\"class\": \"searchby unstriped scroll\"})\n",
        "    links = []\n",
        "    for table in soup.findAll('table', {'class': 'searchby unstriped scroll'}):\n",
        "        for tr in table.findAll('tr'):\n",
        "            for a in tr.findAll('a'):\n",
        "                if '/vuls/id/' in a['href']:\n",
        "                    links.append(a['href'])\n",
        "\n",
        "    return links\n",
        "\n",
        "\n",
        "def crawl_page(link):\n",
        "    global list_of_att\n",
        "    try:\n",
        "        r = requests.get('https://www.kb.cert.org'+link,proxies={'http':f'localhost:{random.randint(5002,5016)}','https':f'localhost:{random.randint(5002,5016)}'})\n",
        "    except requests.exceptions.ProxyError:\n",
        "        r = requests.get('https://www.kb.cert.org' + link, proxies={'http': f'localhost:{random.randint(5002, 5016)}',\n",
        "                                                                    'https': f'localhost:{random.randint(5002, 5016)}'})\n",
        "    parsed_article = BeautifulSoup(r.text, \"lxml\")\n",
        "    paragraphs = parsed_article.find_all('p')\n",
        "    headers = parsed_article.find_all('h3')\n",
        "    title = parsed_article.find('h2', {'class': 'subtitle'})\n",
        "    dict = {}\n",
        "    dict['Title'] = title.text\n",
        "    for header in parsed_article.find_all('h3'):\n",
        "        para = header.find_next_sibling('p')\n",
        "        try:\n",
        "            dict[header.text] = para.text\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    list_of_att.append(dict)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for i in range(1,230):\n",
        "        print(f'page : {i}')\n",
        "        list_of_link = link_extractor(i)\n",
        "        with ThreadPoolExecutor(max_workers=15) as executor:\n",
        "            for link in list_of_link:\n",
        "                print(link)\n",
        "                executor.submit(crawl_page,link)\n",
        "\n",
        "    with open(\"./content/drive/MyDrive/cert.json\", \"a\", encoding='utf-8') as outfile:\n",
        "        json.dump(list_of_att, outfile, ensure_ascii=False, indent=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "r = requests.get('https://www.kb.cert.org/vuls/bypublished/desc/?page=1')\n",
        "soup = BeautifulSoup(r.text,'lxml')\n",
        "table = soup.find(\"table\", attrs={\"class\":\"searchby unstriped scroll\"})\n",
        "\n",
        "# The first tr contains the field names.\n",
        "headings = [th.get_text() for th in table.find(\"tr\").find_all(\"th\")]\n",
        "print(headings)\n",
        "#datasets = []\n",
        "#for row in table.find_all(\"tr\"):\n",
        " #   datasets = []\n",
        "  #  for td in row.find_all(\"td\"):\n",
        "   #     datasets.append(td.text)\n",
        "    #print(datasets)\n",
        "links = []\n",
        "for table in soup.findAll('table', {'class': 'searchby unstriped scroll'}):\n",
        "    for tr in table.findAll('tr'):\n",
        "        for a in tr.findAll('a'):\n",
        "            if '/vuls/id/' in a['href'] :\n",
        "                links.append(a['href'])\n",
        "\n"
      ],
      "metadata": {
        "id": "cILvYxIsMgrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import bs4 as bs\n",
        "import json\n",
        "\n",
        "r = requests.get('https://www.kb.cert.org/vuls/id/534195')\n",
        "\n",
        "parsed_article = BeautifulSoup(r.text, \"lxml\")\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "headers = parsed_article.find_all('h3')\n",
        "title = parsed_article.find('h2', {'class': 'subtitle'})\n",
        "dict = {}\n",
        "dict['Title'] = title.text\n",
        "\n",
        "table = parsed_article.findAll('div',attrs={\"class\":\"large-12 columns\"})\n",
        "for x in table:\n",
        "    print (x.find_all('p').text)"
      ],
      "metadata": {
        "id": "w0bu7DBrM5IF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}